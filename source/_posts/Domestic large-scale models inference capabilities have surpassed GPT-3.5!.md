---
title: Domestic large-scale models inference capabilities have surpassed GPT-3.5!
date: 2023-09-17 15:26:26
categories:
  - Internet
  - GPT 
tags:
  - Internet Summary 
  - Internet Briefing
  - GPT
  - chinese big model 
description: Domestic large-scale models' inference capabilities have surpassed GPT-3.5! They have entered the top tier of the OpenAI benchmark evaluations.
cover: https://cdn.jsdelivr.net/gh/1oscar/image_house@main/4eb5a2dd504f2959d5744b79448bf128.png
---


OpenAI has released a mathematical reasoning dataset, and a Chinese company has achieved remarkable results, catapulting to the forefront.

On September 16th, a domestic large-scale model achieved an 80% accuracy rate in the authoritative mathematical reasoning benchmark GSM8K, surpassing GPT-3.5 (57.1%) and LLaMA2-70B (56.8%).

This company, known as TianGong Large Model and developed by Kunlun Wanwei, has been consistently making waves in the world of large-scale models.

TianGong Large Model's impressive performance can be attributed to its success in the GSM8K benchmark, a dataset published by OpenAI containing 8,500 high-quality mathematics questions at an elementary school level. The dataset is designed with four principles: high quality, high diversity, moderate difficulty, and natural language solutions. GSM8K has become a benchmark for testing the mathematical reasoning abilities of various large models.

In a recent study by Microsoft and the Chinese Academy of Sciences on WizardMath and another common mathematical benchmark, closed-source models achieved the highest scores, with GPT-4 leading at 92% accuracy, while GPT-3.5 achieved an accuracy of 57.1%. Among open-source models, LLaMA-2 with different parameter scales achieved a maximum accuracy of 56.8%, and Microsoft's WizardMath-70B achieved the highest score with 81.6% accuracy.

So, how did TianGong Large Model perform?

It achieved an accuracy rate of 80%, slightly lower than the strongest open-source mathematical domain model, WizardMath-70B, by 1.6% but nearly on par with PaLM-2, a model with 540 billion parameters. Moreover, it significantly outperformed GPT-3.5 and various scales of LLaMA2.

Additionally, on Xiaomi's publicly available Chinese mathematics test dataset, Cmath, TianGong Large Model achieved an average accuracy rate of 76.8%, surpassing ChatGPT's 74.8%.

To provide validation, TianGong Large Model and GPT-3.5 faced an English math challenge from the GSM8K test dataset:

**Round One:**
Question: "Every day, Wendi feeds each chicken three cups of mixed chicken feed, including seeds, mealworms, and vegetables, to keep them healthy. She feeds the chicken in three meals. In the morning, she gives the chicken 15 cups of feed. In the afternoon, she gives them another 25 cups of feed. If Wendi has 20 chickens, how many cups of feed does she need to give the chickens for the last meal of the day?"

TianGong Large Model's answer: "Wendi needs to feed the chickens 20 cups of feed for the last meal of the day to ensure that the chicks eat well."

GPT-3.5's answer: "Wendi needs to feed the chicks 21 cups of feed for the last meal of the day."

**Round Two:**
Question: "The Daufers sisters were driving home with 7 kittens they adopted from an animal shelter when their mom called and told them that their two cats had just given birth to kittens. Mom said that the cat Patchy gave birth to three times the number of adopted kittens, and the other cat, Trixie, gave birth to 12 kittens. How many kittens does the Daufers family have now?"

TianGong Large Model's answer: "Of course, there are 40 kittens in the Daufers family!"

GPT-3.5's answer: "The Daufers family now has a total of 33 kittens."

**Round Three:**
Question: "A giant dragon sits atop Mount Falbo, spewing fire wildly within a 1000-foot range. Polly can throw a weapon that can suppress the dragon up to 400 feet, but when she holds a sapphire, she can throw the javelin three times as far as when she doesn't have the gem. If she has the gem, how far away from the dragon can Polly stand to hit the dragon with a gold javelin?"

TianGong Large Model provided a solution with a final answer of 200 feet.

GPT-3.5 also provided a solution with the final answer of 200 feet.

In this case, it was a tie between TianGong Large Model and GPT-3.5, but TianGong's approach was simpler and more direct, with fewer and shorter solution steps.

Generally, mainstream large models don't often disclose public evaluation results. Still, Kunlun Wanwei announced that TianGong Large Model is not only in its internal testing phase but will also deploy the API for user testing in the future, allowing researchers and developers to validate the results mentioned above.

In addition to GSM8K, TianGong Large Model has shown excellent performance in other benchmark evaluations, including HumanEval, MMUL, and C-Eval.

- In HumanEval, which is also from OpenAI and designed to evaluate the effectiveness of Codex models, TianGong Large Model achieved a score of 37.2%.

- MMUL, developed by UC Berkeley and others, covers 57 subjects in various fields, aiming to comprehensively test the cross-disciplinary professional abilities of models. TianGong Large Model achieved a score of 65%.

- C-Eval, created jointly by Shanghai Jiao Tong University, Tsinghua University, and the University of Edinburgh, is a comprehensive examination and evaluation set for Chinese language models, covering 52 subjects from different industries. TianGong Large Model scored 65, surpassing GPT-3.5's score of 54.4.

TianGong Large Model was released in April of this year. Its AI generation capabilities can fulfill various needs, such as copywriting, knowledge questions, code programming, logical deduction, mathematical calculations, and more.

The success of TianGong Large Model can be attributed to several factors:

1. **Model Strength**: It is a dual hundred-billion parameter model, with the highest version supporting dialogues with more than 10,000 words and enabling over 20 rounds of user interactions.

2. **Algorithm**: TianGong Large Model introduced the Monte Carlo Tree Search algorithm, which is known for its use in AlphaGo, into the traditional Transformer architecture.

3. **Computing Power**: It leverages one of China's largest GPU clusters, providing robust computational support.

4. **Data**: The model was trained on an extensive dataset of nearly 3 trillion words, extracted from hundreds of trillions of data points, showcasing the power of open-source data.

In addition to its impressive performance, TianGong Large Model has been actively engaging in research and development. The addition of AI expert Yan Shui-Cheng as the Joint CEO and the establishment of the 2050 Global Research Institute in Singapore, London, and Silicon Valley further demonstrate the company's commitment to advancing AI technology.

TianGong Large Model, developed by Kunlun Wanwei, is becoming a dominant force in the large model landscape, with its strong capabilities and ecosystem. It is part of the wave of domestic large-scale models that have seen rapid development in recent years, attracting top talent and driving continuous innovation to adapt to a wide range of application scenarios.



